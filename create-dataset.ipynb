{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82dfda5-dec4-431e-96f2-ccccd98e9b24",
   "metadata": {},
   "source": [
    "# Creating HF Dataset for Mistral Fine-tuning\n",
    "\n",
    "Code authored by: Shaw Talebi <br>\n",
    "Video link: https://youtu.be/XpoKB3usmKc <br>\n",
    "Blog link: https://medium.com/towards-data-science/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32 <br>\n",
    "<br>\n",
    "Colab link: https://colab.research.google.com/drive/1AErkPgDderPW0dgE230OOjEysd0QV1sR?usp=sharing <br>\n",
    "Dataset link: https://huggingface.co/datasets/shawhin/shawgpt-youtube-comments <br>\n",
    "Model link: https://huggingface.co/shawhin/shawgpt-ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32125c58-b985-4083-9431-ce5ae3d35b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hussen/Desktop/Year 4/Semester 2/IL181/Assignments/Assignment 2/nlp/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/hussen/Desktop/Year 4/Semester 2/IL181/Assignments/Assignment 2/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dbd38c-9f4f-4697-9ea0-e52c2912c45b",
   "metadata": {},
   "source": [
    "### prep training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdaaa4fa-7550-40e6-bd4a-83250b6783cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv of YouTube comments\n",
    "comment_list = []\n",
    "response_list = []\n",
    "\n",
    "with open('data/reddit-comments.csv', mode ='r') as file:\n",
    "    file = csv.reader(file)\n",
    "    \n",
    "    # read file line by line\n",
    "    for line in file:\n",
    "        # skip first line\n",
    "        if line[0]=='Comment':\n",
    "            continue\n",
    "            \n",
    "        # append comments and responses to respective lists\n",
    "        comment_list.append(line[0])\n",
    "        response_list.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e327ecb-6421-4040-9113-e591fc81c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt 1 at prompt format\n",
    "# intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "# It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
    "# ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "# thus keeping the interaction natural and engaging.\n",
    "# \"\"\"\n",
    "\n",
    "# example_template = lambda comment, response: f'''<s>[INST] {intstructions_string} \\nViewer: {comment} \\nShawGPT: [/INST]''' + response + \"</s>\"\n",
    "\n",
    "# example_list = []\n",
    "# for i in range(len(comment_list)):\n",
    "#     example = example_template(comment_list[i],response_list[i])\n",
    "#     example_list.append(example)\n",
    "\n",
    "# print(example_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd6a8d1c-be84-40c9-ad2b-52ecc2c54b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] I was going to downvote you for refusing the reasonable request, but since you were impolite back, you get my upvote instead.   \\n  \\n faggot \n",
      "[/INST]\n",
      "that is better</s>\n"
     ]
    }
   ],
   "source": [
    "# attempt 2 at prompt format\n",
    "instructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
    "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "\n",
    "example_template = lambda comment, response: f'''<s>[INST] {comment} \\n[/INST]\\n''' + response + \"</s>\"\n",
    "\n",
    "example_list = []\n",
    "for i in range(len(comment_list)):\n",
    "    example = example_template(comment_list[i],response_list[i])\n",
    "    example_list.append(example)\n",
    "\n",
    "print(example_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4a39e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Yup. Insane payments. Not sustainable. But the banks are all in \n",
      "[/INST]\n",
      "But what if they only give loans to unemployed people? That way, the banks wouldn't have any risk, since 6 x 0 is 0.</s>\n"
     ]
    }
   ],
   "source": [
    "print(example_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfa3e352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] \n",
      "I was going to downvote you for refusing the reasonable request, but since you were impolite back, you get my upvote instead.   \\n  \\n faggot \n",
      "[/INST]\n",
      "that is better</s>\n"
     ]
    }
   ],
   "source": [
    "instructions_string = f\"\"\"FunGPT is an engaging and witty AI assistant trained on Reddit comment data. Its responses are designed \n",
    "to be entertaining, humorous, and unpredictable, while still being relevant to the user's comments. FunGPT communicates in a casual, \n",
    "conversational tone, often employing sarcasm, wordplay, and pop culture references. It aims to keep interactions light-hearted and fun, \n",
    "occasionally pushing boundaries with edgy or risqué humor (but never crossing ethical lines).\n",
    "\n",
    "Please respond to the following comment in an engaging and entertaining way.\n",
    "\"\"\"\n",
    "\n",
    "example_template = lambda comment, response: f'''<s>[INST] \\n{comment} \\n[/INST]\\n''' + response + \"</s>\"\n",
    "\n",
    "example_list = []\n",
    "for i in range(len(comment_list)):\n",
    "    example = example_template(comment_list[i], response_list[i])\n",
    "    example_list.append(example)\n",
    "\n",
    "print(example_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2806b44-2b2d-4a60-bb53-635bbb563f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test split\n",
    "test_index_list = random.sample(range(0, len(example_list)-1), 9)\n",
    "\n",
    "test_list = [example_list[index] for index in test_index_list]\n",
    "\n",
    "for example in test_list:\n",
    "    example_list.remove(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931860be-fbd2-48cd-9d62-382981476c6e",
   "metadata": {},
   "source": [
    "### create HF dataest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fa95664-1ea8-4296-aea0-1bdb2f9d37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict({'train':Dataset.from_dict({\"example\":example_list}), 'test':Dataset.from_dict({\"example\":test_list})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a586cfe4-7206-4f27-9c55-6e120bfed587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['example'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['example'],\n",
       "        num_rows: 9\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03f4b83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 164.55ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1360.46ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/hussenmi/reddit_comments/commit/16b0f165df840b5c589c0f11b894263e173c1718', commit_message='Upload dataset', commit_description='', oid='16b0f165df840b5c589c0f11b894263e173c1718', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'reddit_comments'\n",
    "data.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07acbcf3-f71a-4180-9148-067bc4a01af2",
   "metadata": {},
   "source": [
    "### push dataset to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "237d6a3e-74c0-4f36-8f3d-20a2cc1e9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: notebook login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# # option 2: key login\n",
    "# from huggingface_hub import login\n",
    "# write_key = 'hf_' # paste token here\n",
    "# login(write_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bebbecf7-5603-4ba4-8cf9-8160af5df57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3120aebadc7944ea9365a79aecee5f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a26c85da8004e1d9e1447461b53a8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d403e1ce4f42b88970d168dd4b4e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a201b292ca58425d934ae7e717ed3497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec4fb2cfcb5411ba615f94a3f280d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/shawhin/shawgpt-youtube-comments/commit/eb6e890103c25bb7f4be2d8ce541dd2b320d46f9', commit_message='Upload dataset', commit_description='', oid='eb6e890103c25bb7f4be2d8ce541dd2b320d46f9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push dataset to hub\n",
    "data.push_to_hub(\"shawhin/shawgpt-youtube-comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc2fbe-9f2b-436e-b28b-b827b84fe858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
