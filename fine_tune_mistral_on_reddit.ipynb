{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6986c142e7134cc186d0e7d935225d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a2dfd68947d4f43a4db18909c0992ca",
              "IPY_MODEL_3f2c7e87071c422f96a0c99c14866a70",
              "IPY_MODEL_ff57cd3f35eb4161a783eaacf375ff97"
            ],
            "layout": "IPY_MODEL_45fe8b92dbac4db390ab8b9d16f8e574"
          }
        },
        "8a2dfd68947d4f43a4db18909c0992ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e2adb2955b645d1bd2289c23794da50",
            "placeholder": "​",
            "style": "IPY_MODEL_cdfa7b06e60e40818eac7743a22f0873",
            "value": "Map: 100%"
          }
        },
        "3f2c7e87071c422f96a0c99c14866a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f6da03518c84fd58e7640de699e76f1",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69cfbc27682b40868073f589145bd455",
            "value": 9
          }
        },
        "ff57cd3f35eb4161a783eaacf375ff97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d02a638ab8e74765b07c65c45031a3cf",
            "placeholder": "​",
            "style": "IPY_MODEL_46cfae1523d445d692d881876535f6ec",
            "value": " 9/9 [00:00&lt;00:00, 199.86 examples/s]"
          }
        },
        "45fe8b92dbac4db390ab8b9d16f8e574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2adb2955b645d1bd2289c23794da50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdfa7b06e60e40818eac7743a22f0873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f6da03518c84fd58e7640de699e76f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69cfbc27682b40868073f589145bd455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d02a638ab8e74765b07c65c45031a3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46cfae1523d445d692d881876535f6ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Mistral-7b-Instruct On Reddit Comment and Reply Data"
      ],
      "metadata": {
        "id": "-7hYqUsxYQtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the chatbots we have today have been aligned by their creators to behave in a certain way. I want an AI chatbot that is fun and engaging. The chatbots we have including ChatGPT and Gemini are aligned in a way that they only give answers deemed as appropriate. They also have a specific tone (although this could be somewhat altered using prompt engineering). What I want from my chatbot is unfiltered thoughts and for it to give me its opinion when asked about something, instead of giving the usual response of, __\"I'm an AI assistant. I don't have opinions, but I can guide you ...\"__\n",
        "\n",
        "To do this, I am going to fine-tune an open-source large language model. The model I'll use is the Mistral-7b-Instruct model developed by the Mistral team. It's freely available on Hugging Face ([model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GPTQ)). The paper outlining the details of the model can be found [here](https://arxiv.org/abs/2310.06825). And for the data, I'll be using Reddit comments and replies to those comments. These comments are scraped from the wallstreetbets subreddit, and the dataset is available on Hugging Face as well ([dataset](https://huggingface.co/datasets/Sentdex/wsb_reddit_v002)). The dataset is composed of 118k rows, but I'm not using all of them. There are a couple of reasons for this: the first is that using that much data requires a lot of computational power, which is a constraint, and secondly, the method I'm using to fine tune the model works with very few examples, and that's one of the beauty of this method. The final version of model is fine tuned with only 250 examples, but in earlier iterations, I fine-tuned it with even fewer examples, and I was still able to see results. So we can see that the method I'm using is a really practical method of acheiving good results with a small training data and few computational resources.\n",
        "\n",
        "The dataset was originally in a JSON format, and I changed it to a CSV format that is compatible with how my pipeline is set up. The cleaned data is in the file `reddit-comments.csv`."
      ],
      "metadata": {
        "id": "3CBx5KXizK1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start with, let's first load the base model that we'll be using and ask it some questions to see what kind of response we can get from it. The model is available on Hugging Face and we can use the `transformers` library to load it. Other necessary imports are also included here."
      ],
      "metadata": {
        "id": "VpEYGB9rzpnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq\n",
        "!pip install optimum\n",
        "!pip install bitsandbytes\n",
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "XeaOnjP2zsy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import transformers\n",
        "\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "tfSl5xRs0y8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get an idea of what kind of responses the base model gives, let's load it and try it out with some prompts."
      ],
      "metadata": {
        "id": "zJpEDerSH-KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "VVstnrh-0m2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "\n",
        "# Here, CausalLM is used becuase the model is for generating text (like GPT). It'd be MaskedLM for models like BERT.\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")"
      ],
      "metadata": {
        "id": "GFcKal6c96su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc14fd75-69b0-4f49-e965-dcb46ae8e541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading in the model, we need to also get a tokenizer that'll convert our prompt to tensors. The exact details of the tokenizer are not mentioned in the paper. But here, we can just load it for our pretrained model diretly from Hugging Face as well using the `AutoTokenizer` class"
      ],
      "metadata": {
        "id": "eC7L-tx_0hWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load tokenizer"
      ],
      "metadata": {
        "id": "bCKC3_yl0pNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "c3CIa8C80Vtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Base Model"
      ],
      "metadata": {
        "id": "VnFiyBOc329l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first try to use the base model. I am assuming that when we ask it questions that requires having an opinion, it backs off not to offend anyone. Most of the current LLMs, including ChatGPT, are aligned this way, and I just want something fun and lively to converse with. So let's start off with the base model and see what we get."
      ],
      "metadata": {
        "id": "572svP4x92Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt, repetition_penalty=1.5, do_sample=True, max_new_tokens=140, specific_commands=None):\n",
        "    \"\"\"\n",
        "    Generate a response based on the given prompt using the pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt for generating the response.\n",
        "        repetition_penalty (float, optional): The repetition penalty to apply during generation. Defaults to 1.5.\n",
        "        do_sample (bool, optional): Whether to use sampling during generation. Defaults to True.\n",
        "        max_new_tokens (int, optional): The maximum number of new tokens to generate. Defaults to 140.\n",
        "        specific_commands (str, optional): Specific commands to include in the prompt. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response based on the given prompt.\n",
        "    \"\"\"\n",
        "    model.eval() # Put the model in evaluation mode (dropout modules are deactivated)\n",
        "\n",
        "    # Since the base model we're using is the Mistral-7b-Instruct, and it's an instruction-tuned model, it expects the prompt to be in a specific format.\n",
        "    # It expects the [INST] and [/INST] start and end tokens. They are special tokens used by the model. So that's why we're adjusting the prompts that way.\n",
        "    if specific_commands:\n",
        "        prompt = f\"[INST] \\n{specific_commands} \\n{prompt} \\n[/INST]\"\n",
        "    else:\n",
        "        prompt = f\"[INST] \\n{prompt} \\n[/INST]\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=max_new_tokens, repetition_penalty=repetition_penalty, do_sample=do_sample)\n",
        "    return tokenizer.batch_decode(outputs)[0]"
      ],
      "metadata": {
        "id": "q27w5ULO4jbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment = \"What do you think about the US politics?\"\n",
        "response = generate_response(comment)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Du9KLy3N8Ap6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922aba4a-9143-49bb-de5b-588ef3924d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST] \n",
            "What do you think about the US politics? \n",
            "[/INST] I don't have personal experiences or opinions. However, I can provide information and analysis on U.S. politics based on data and facts. The political climate in the United States is complex and multifaceted. There are two major national parties: the Democratic Party and the Republican Party. Political discourse often centers around issues like economy, healthcare, immigration, foreign policy, education, environment, social justice, gun rights, among others. Currently, there is significant polarization between thesetwopartiesandtheirsupportersbasedonideologyaswellasthetraditionalred- vs.-blue divide. Recent yearshaveseenincreasingintensityinsocialissueslikeimm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the output of the model, it's saying that it doesn't have any opinions and just overall gives a very basic response, which is fine, but I wanted something fun for this chatbot. To achieve that, like mentioned earlier, we're going to fine-tune it using our Reddit data."
      ],
      "metadata": {
        "id": "4FUVSvd4ATXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a couple of different methods to fine-tune an LLM. One common method is full fine-tuning. The process results in a new version of the model with updated weights. One caveat with this process is that full fine-tuning requires enough memory and computing power to process all the gradients and other components being updated during training.\n",
        "\n",
        "In order to work against this constraint, we have another method called parameter-efficient fine-tuning. In this method, we only update a small set of parameters, which saves us a lot of computational power and memory. One method of doing this is called **LoRA (Low-Rank Adaptation)**."
      ],
      "metadata": {
        "id": "GKofOJiXo8vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoRA\n",
        "\n",
        "LoRA (Low-Rank Adaptation) is a method used for parameter-efficient fine-tuning of large language models. It is designed to update only a small set of parameters, reducing the computational power and memory requirements compared to full fine-tuning.\n",
        "\n",
        "The idea behind LoRA is to identify a subset of parameters in the model that can be updated to adapt the model to a specific task or domain. This subset of parameters is referred to as the \"target modules.\" By updating only these target modules, LoRA achieves parameter-efficient fine-tuning.\n",
        "\n",
        "The key concept in LoRA is the low-rank approximation of the weight matrices in the target modules. Instead of updating the full weight matrices, LoRA decomposes them into low-rank factors. This decomposition reduces the number of parameters that need to be updated, resulting in significant memory and computational savings.\n",
        "\n",
        "During the fine-tuning process, LoRA updates the low-rank factors of the target modules using gradient descent. The gradients are computed using backpropagation through the model, similar to traditional fine-tuning methods. However, since LoRA only updates a small set of parameters, the computational cost is significantly reduced."
      ],
      "metadata": {
        "id": "LcEvQGTJ_1t1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's discuss this using an example. In class, when discussing transformers, we talked about the three vectors that are generated in each head: $Q, K$, and $V$. In order to generate these vectors, we have matrices associated with each of them: $W^Q, W^K$, and $W^V$. Since in LLMs, we have multiple attention heads, we'll also have more of these matrices as well. For illustrating what LoRA does, lets just look at one of the matrices, $W^Q$.\n",
        "\n",
        "For our discussion, let's assume this weight matrix is a $7$ x $7$ matrix. We have 49 elements. This is an example of what the matrix could look like:\n",
        "\n",
        "$$\n",
        "W^Q = \\begin{pmatrix}\n",
        "4 & 7 & 2 & 9 & 1 & 5 & 3 \\\\\n",
        "6 & 3 & 8 & 2 & 7 & 4 & 1 \\\\\n",
        "5 & 9 & 1 & 3 & 8 & 6 & 2 \\\\\n",
        "7 & 2 & 4 & 6 & 9 & 1 & 5 \\\\\n",
        "3 & 8 & 6 & 1 & 4 & 2 & 7 \\\\\n",
        "2 & 5 & 9 & 7 & 3 & 8 & 6 \\\\\n",
        "1 & 6 & 3 & 4 & 5 & 7 & 9 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "If we want to use full fine-tuning, we'd have to update all of these elements. And we can assume as the matrix gets bigger, we need to update more values as well. One idea to reduce the amount of elements to update is, instead of having one big matrix, why don't we have two smaller matrices, when multiplied will give us the same dimension as the original matrix. Let's call these two matrices $A$ and $B$, and in our scenario, their dimensions will be $7$ x $r$ and $r$ x $7$ simultaneously. These matrices are lower in rank when compared to the original matrix. They have a rank of $r$. So they could, for example, be a $7$ x $2$ and a $2$ x $7$ matrix. This is what they could look like:\n",
        "\n",
        "$$\n",
        "A = \\begin{pmatrix}\n",
        "a_{11} & a_{12} \\\\\n",
        "a_{21} & a_{22} \\\\\n",
        "a_{31} & a_{32} \\\\\n",
        "a_{41} & a_{42} \\\\\n",
        "a_{51} & a_{52} \\\\\n",
        "a_{61} & a_{62} \\\\\n",
        "a_{71} & a_{72} \\\\\n",
        "\\end{pmatrix}\n",
        "$$,\n",
        "\n",
        "$$B = \\begin{pmatrix}\n",
        "b_{11} & b_{12} & b_{13} & b_{14} & b_{15} & b_{16} & b_{17} \\\\\n",
        "b_{21} & b_{22} & b_{23} & b_{24} & b_{25} & b_{26} & b_{27} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "When we multiply them, we get a $7$ x $7$ matrix. This matrix is called the adapter. It could be element-wise added to the original weight matrix, $W^Q$ and the result will be the new fine-tuned version of the weight matrix that we can use in our model. Apparently, this would work as a fine-tuning method because the values in these low-rank matrices, $A$ and $B$ are learned during the fine-tuning process, so they have the new information from the new data. When we add this new matrix to original matrix, we have both the pre-trained information and the new information extracted from the fine-tuning data."
      ],
      "metadata": {
        "id": "IUVzT41zgEkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this is great, but does it really save us that much computational power and memory? Let's run some numbers to figure that out. In the previous example, we had a $7$ x $7$ matrix, which had $49$ elements. When we used the two low-rank matrices, we had $7$ x $2 = 14$ elements each, and since we had two matrices, we had $28$ elements we needed to update. But what happens as we scale up?\n",
        "\n",
        "Let's assume our original weight matrix is $d$ x $d$ elements, and our low-rank matrices are $d$ x $r$ and $r$ x $d$.\n",
        "\n",
        "1. The total number of elements we have for the orginal matrix is $d^2$ and for the combination of the two low-rank matrices, it's $2$ x $d$ x $r$. WHen $r$ is much smaller than $d$, which is usually the case, $2dr$ is significantly less than $d^2$, meaning fewer parameters need to be learned and updated during training.\n",
        "2. The matrices $A$ and $B$ also need to be multiplied to get the adapter to be added to the original weight matrix. But even this process is a lot faster because of the matrices' size. When multiplying $A$ and $B$, we take the dot product of the row of $A$ and the column of $B$ to get the value for a single entry in $AB$. Since the number of columns of $A =$ number of rows of $B=r$, the cost of getting a single entry in $AB$ is $r$. Since the size of $AB=d^2$, the cost of mutiplying $A$ and $B$ is $O(d^2r)$. This is much more efficient than operations involving matrices of size $d$ x $d$ when $r<d$."
      ],
      "metadata": {
        "id": "gN-aohgRlbat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is great and saves us a lot of computational work and allows us to fine-tune our model efficiently. But lucky for us, researchers have also come up with a way to make this even more efficient by adding quantization to the mix to create **QLoRA (Quantized Low-Rank Adaptation)**.\n",
        "\n",
        "Quantization is a process that reduces the precision of numerical values to save memory and computational resources. This could involve rounding, clustering values, and mapping to a set of representable values, while preserning as much of the original information as possible."
      ],
      "metadata": {
        "id": "8CZ4oWNSpMJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, I'll be using QLoRA to fine-tune the base model. To do that, I'll be using the `peft` library from Hugging Face. To do that, let's first load in our data. I uploaded the cleaned dataset to my Hugging Face account, so I'll load it from there."
      ],
      "metadata": {
        "id": "0N1eOuQAuEir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def get_dataset(username, dataset_name):\n",
        "    # load dataset\n",
        "    data = load_dataset(f\"{username}/{dataset_name}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "username = 'hussenmi'\n",
        "dataset_name = 'reddit_comments'\n",
        "data = get_dataset(username, dataset_name)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgkJCWrtwXnM",
        "outputId": "76178723-a9ff-4bb8-9fde-4a7b68051fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['example'],\n",
              "        num_rows: 240\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['example'],\n",
              "        num_rows: 9\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at some of the data\n",
        "for i in range(5):\n",
        "    print(data[\"train\"][i][\"example\"])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TfR7_FY06kT",
        "outputId": "e56efc8a-765f-4370-8fa9-43a4a69ab2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] \n",
            "Yup. Insane payments. Not sustainable. But the banks are all in \n",
            "[/INST]\n",
            "But what if they only give loans to unemployed people? That way, the banks wouldn't have any risk, since 6 x 0 is 0.</s>\n",
            "\n",
            "<s>[INST] \n",
            "Did the run out of water there yet? \n",
            "[/INST]\n",
            "IIRC they have in Cape Town</s>\n",
            "\n",
            "<s>[INST] \n",
            "Thats the same thing?    \\n  Edit: learned the difference \n",
            "[/INST]\n",
            "^ This guy doesn't get the joke.</s>\n",
            "\n",
            "<s>[INST] \n",
            "Originally it wasn't. I marked it using my secret mod powers. \n",
            "[/INST]\n",
            "Nice</s>\n",
            "\n",
            "<s>[INST] \n",
            "Pembrolizumab is the single defining factor of Merck right now. The entirety of stock movement is based around this single immunotherapy.    \\n  If pembrolizumab can consistently continue to show that it is a better PD-1 checkpoint inhibitor over competitors, which so far, it generally has managed to do, Merck will continue to do well. \n",
            "[/INST]\n",
            "Yea Keytruda is really important for them, but it's kind of disappointing that they don't have multiple irons in the fire currently.</s>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we get the data, we need to tokenize our data and also get a data collator that adds padding so that the shape of our matrices match (just like what happens in padding in Convolutional Neural Nets)."
      ],
      "metadata": {
        "id": "OJBGbOIqwkaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenized_data(examples):\n",
        "    \"\"\"Tokenizes the input text and returns the tokenized inputs.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A dictionary containing the input examples.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the tokenized inputs.\n",
        "    \"\"\"\n",
        "    # extract text\n",
        "    text = examples[\"example\"]\n",
        "\n",
        "    # tokenize and truncate text\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "def prepare_data(data):\n",
        "    \"\"\"Prepares the data for training.\n",
        "\n",
        "    Args:\n",
        "        data (Dataset): The input dataset to be prepared.\n",
        "        tokenizer (Tokenizer): The tokenizer used to tokenize the data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dataset, DataCollator]: A tuple containing the tokenized data and the data collator.\n",
        "    \"\"\"\n",
        "    # tokenize training and validation datasets\n",
        "    tokenized_data = data.map(get_tokenized_data, batched=True)\n",
        "\n",
        "    # setting pad token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # data collator\n",
        "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "    return tokenized_data, data_collator\n",
        "\n",
        "\n",
        "tokenized_data, data_collator = prepare_data(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6986c142e7134cc186d0e7d935225d30",
            "8a2dfd68947d4f43a4db18909c0992ca",
            "3f2c7e87071c422f96a0c99c14866a70",
            "ff57cd3f35eb4161a783eaacf375ff97",
            "45fe8b92dbac4db390ab8b9d16f8e574",
            "1e2adb2955b645d1bd2289c23794da50",
            "cdfa7b06e60e40818eac7743a22f0873",
            "3f6da03518c84fd58e7640de699e76f1",
            "69cfbc27682b40868073f589145bd455",
            "d02a638ab8e74765b07c65c45031a3cf",
            "46cfae1523d445d692d881876535f6ec"
          ]
        },
        "id": "KUaZNJhQxDj0",
        "outputId": "3f8e4494-a74b-4e1c-aee4-cbb5e2c3f5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6986c142e7134cc186d0e7d935225d30"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we get the data and prepare it accordingly, we need to prepare our model for training. This means setting it to `train` mode, reducing the number of trainable parameters using the QLoRA method we just discussed, and other things like enabling gradient checkpointing in order to save memory at the cost of more computation."
      ],
      "metadata": {
        "id": "bRUx2cZ6zO0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model_for_training(model):\n",
        "    \"\"\"Prepare the model for training.\n",
        "\n",
        "    This function prepares the given model for training by performing the following steps:\n",
        "    1. Puts the model in training mode (activates dropout modules).\n",
        "    2. Enables gradient checkpointing.\n",
        "    3. Enables quantized training.\n",
        "    4. Configures LoRA with the specified parameters.\n",
        "    5. Creates a trainable version of the model using the LoRA configuration.\n",
        "    6. Prints the percentage of trainable parameters as compared to the original parameters.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to be prepared for training.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The prepared model.\n",
        "\n",
        "    \"\"\"\n",
        "    model.train() # Put the model in training mode (dropout modules are activated)\n",
        "\n",
        "    # enable gradient check pointing\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # enable quantized training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # LoRA config\n",
        "    config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # LoRA trainable version of model\n",
        "    model = get_peft_model(model, config)\n",
        "\n",
        "    # trainable parameter count\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    return model\n",
        "\n",
        "model = prepare_model_for_training(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avpZkr-UGDko",
        "outputId": "a783b910-bb00-4da0-b017-6378de1175d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7928519441906561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have both our model and data ready, we can fine-tune it."
      ],
      "metadata": {
        "id": "uCpfsZlQGlT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_model(model, tokenized_data, data_collator, training_args):\n",
        "    \"\"\"Fine-tunes the model on the given tokenized data.\n",
        "\n",
        "    Args:\n",
        "        model (PreTrainedModel): The model to be fine-tuned.\n",
        "        tokenized_data (Dataset): The tokenized training and validation data.\n",
        "        data_collator (DataCollator): The data collator to be used for training.\n",
        "        training_args (TrainingArguments): The training arguments to be used for training.\n",
        "        compute_metrics (Callable): The function used to compute the evaluation metrics.\n",
        "\n",
        "    Returns:\n",
        "        transformers.Trainer: The trainer used for fine-tuning the model.\n",
        "    \"\"\"\n",
        "    # configure trainer\n",
        "    trainer = transformers.Trainer(\n",
        "        model=model,\n",
        "        train_dataset=tokenized_data[\"train\"],\n",
        "        eval_dataset=tokenized_data[\"test\"],\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # train model\n",
        "    model.config.use_cache = False  # silence the warnings\n",
        "    trainer.train()\n",
        "\n",
        "    model.config.use_cache = True\n"
      ],
      "metadata": {
        "id": "lAniLyqF1X6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now fine-tune the data by calling the function with the parameters we have gathered:"
      ],
      "metadata": {
        "id": "4DLNTnZZKBs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "lr = 2e-4\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "\n",
        "# define training arguments\n",
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir= \"fungpt-ft\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=2,\n",
        "    fp16=False,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "finetune_model(model, tokenized_data, data_collator, training_args)#, compute_metrics=compute_metrics)"
      ],
      "metadata": {
        "id": "m6wZK62bJKsJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "outputId": "6541dc49-cbae-4051-ada9-e67cde6eaa54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 30:55, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.251900</td>\n",
              "      <td>3.883362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.608400</td>\n",
              "      <td>3.463136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.180700</td>\n",
              "      <td>3.189507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.921000</td>\n",
              "      <td>3.116976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.828700</td>\n",
              "      <td>3.093508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.732800</td>\n",
              "      <td>3.080739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.661100</td>\n",
              "      <td>3.084115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.640300</td>\n",
              "      <td>3.096904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.592500</td>\n",
              "      <td>3.101039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.550000</td>\n",
              "      <td>3.104319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We have now trained our model. Next, we'll upload the adapters to Hugging Face so that we can easily load them later and add them to the base model to create our fine tuned model. I'll show how to load the fine-tuned model and use it in another notebook. I'll also calculate some metrics in that notebook as well."
      ],
      "metadata": {
        "id": "N8vXUbI54cNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "Xe-atBOM649W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"hussenmi/fungpt-ft\"\n",
        "\n",
        "model.push_to_hub(model_id)\n",
        "trainer.push_to_hub(model_id)"
      ],
      "metadata": {
        "id": "HsnyqXpC67n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ntuuw9MGRnzu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}